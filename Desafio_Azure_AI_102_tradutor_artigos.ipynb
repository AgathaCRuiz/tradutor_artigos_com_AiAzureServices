{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMdnwIMrKD1xNeaKmAVO1/q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","---\n","\n","üìò **Desafio de Projeto: [Tradutor de Artigos T√©cnicos com Azure AI](https://web.dio.me/project/tradutor-de-artigos-tecnicos-com-azureai/learning/d09cfb84-6309-4e9d-ab5b-6a3817f0180c)**\n","\n","Este projeto √© um desafio educacional proposto para desenvolver uma ferramenta de tradu√ß√£o autom√°tica de artigos usando a **Intelig√™ncia Artificial da Microsoft Azure**. Com o **Azure Translator** e o **Azure OpenAI**, o projeto realiza tradu√ß√µes precisas e naturais de textos, aproveitando tecnologias de ponta para processamento de linguagem natural.\n","\n","**Tecnologias:**  \n","- Microsoft Azure Translator para tradu√ß√£o autom√°tica.  \n","- Azure OpenAI para processamento avan√ßado de linguagem.  \n","- Google Colab para desenvolvimento em nuvem.\n","\n","**Objetivo Educacional:**  \n","Desenvolver habilidades pr√°ticas em IA e nuvem, com foco em tradu√ß√µes autom√°ticas e NLP, explorando o ecossistema Azure.\n","\n","---"],"metadata":{"id":"W1KSYM7pVk9r"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Criando Camada de Tradu√ß√£o com o Azure Translator\n","\n","---\n"],"metadata":{"id":"UGD8LVjkWcuQ"}},{"cell_type":"code","source":["pip install requests python-docx\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9m3koS_-xLK","executionInfo":{"status":"ok","timestamp":1730838257271,"user_tz":180,"elapsed":5810,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"outputId":"75b2e500-49ae-4078-b31c-ea68852a4095"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n","Collecting python-docx\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n","Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-docx\n","Successfully installed python-docx-1.1.2\n"]}]},{"cell_type":"code","source":["import requests\n","from docx import Document\n","import os\n","\n","subscription_key = \"4gwXToH0suZJHEKhEl7bIEQfCFUcWmhOLxf4Ct18X0Hmp54liwZpJQQJ99AKACHYHv6XJ3w3AAAbACOGpNfp\"\n","endpoint = \"https://api.cognitive.microsofttranslator.com/\"\n","location = \"eastus2\"\n","language_destination = 'pt-br'\n","\n","def translator_text(text, target_language):\n","  path = '/translate'\n","  constructed_url = endpoint + path\n","  headers = {\n","      'Ocp-Apim-Subscription-Key': subscription_key,\n","      'Ocp-Apim-Subscription-Region': location,\n","      'Content-type': 'application/json',\n","      'X-ClientTraceId': str(os.urandom(16))\n","  }\n","\n","  # The 'text' variable should be accessed here within the function scope\n","  body = [{\n","      'text': text\n","  }]\n","\n","  params = {\n","      'api-version': '3.0',\n","      'from': 'en',\n","      'to': target_language\n","  }\n","\n","  resquest = requests.post(constructed_url, headers=headers, json=body, params=params) # Added params to the request\n","  response = resquest.json()\n","  return response[0]['translations'][0]['text']"],"metadata":{"id":"PEO28gHnASR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translator_text(\"I know you're somewhere out there, somewhere far away\", language_destination )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"7daCO4ZlKW6n","executionInfo":{"status":"ok","timestamp":1730838273175,"user_tz":180,"elapsed":960,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"outputId":"72d7c3fd-a49b-4409-a8b1-9c8a8326a992"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Eu sei que voc√™ est√° em algum lugar l√° fora, em algum lugar distante'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["def translate_document(path):\n","    document = Document(path)\n","    full_text = []\n","    for paragraph in document.paragraphs:\n","        translated_text = translator_text(paragraph.text, language_destination)\n","        full_text.append(translated_text)\n","\n","    translated_doc = Document()\n","    for line in full_text:\n","        print(line)\n","        translated_doc.add_paragraph(line)\n","    path_translated = path.replace(\".docx\", f\"_{language_destination}.docx\")\n","    translated_doc.save(path_translated)\n","    return path_translated\n","\n","input_file = \"/content/musica.docx\"\n","translate_document(input_file)\n"],"metadata":{"id":"vVjPlAJGNkKP","executionInfo":{"status":"ok","timestamp":1730839410025,"user_tz":180,"elapsed":2470,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"colab":{"base_uri":"https://localhost:8080/","height":608},"outputId":"8f0193c3-aae9-4919-eab4-e96e07388009"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Eu sei que voc√™ est√° em algum lugar l√° fora, em algum lugar distante\n","Eu quero voc√™ de volta, eu quero voc√™ de volta\n","Meus vizinhos acham que sou louco, mas n√£o entendem\n","Voc√™ √© tudo que eu tinha, voc√™ √© tudo que eu tinha\n","√Ä noite, quando as estrelas iluminam meu quarto\n","Eu sento sozinho\n","Falando com a lua\n","Tentando chegar at√© voc√™\n","Na esperan√ßa de que voc√™ esteja do outro lado falando comigo tamb√©m\n","Ou sou um tolo que se senta sozinho conversando com a lua?\n","Ah\n","Eu estou me sentindo como se eu fosse famoso, o assunto da cidade\n","Eles dizem que eu enlouqueci, sim, eu enlouqueci\n","Mas eles n√£o sabem o que eu sei, porque quando o sol se p√µe\n","Algu√©m est√° respondendo, sim, eles est√£o respondendo, oh\n","√Ä noite, quando as estrelas iluminam meu quarto\n","Eu sento sozinho\n","Falando com a lua\n","Tentando chegar at√© voc√™\n","Na esperan√ßa de que voc√™ esteja do outro lado falando comigo tamb√©m\n","Ou sou um tolo que se senta sozinho conversando com a lua?\n","Ah, ah, ah\n","Voc√™ j√° me ouviu chamando?\n","(Ah) oh-oh-oh (ah), oh-oh-oh (ah)\n","Porque todas as noites eu estou falando com a lua\n","Ainda tentando chegar at√© voc√™\n","Na esperan√ßa de que voc√™ esteja do outro lado falando comigo tamb√©m\n","Ou sou um tolo que se senta sozinho conversando com a lua?\n","Ah\n","Eu sei que voc√™ est√° em algum lugar l√° fora, em algum lugar distante\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/musica_pt-br.docx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["input_file = \"/content/musica.docx\"\n","translate_document(input_file)"],"metadata":{"id":"mSgSiVpDPkB4","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1730839339320,"user_tz":180,"elapsed":2076,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"outputId":"300d6acf-aef1-4cc1-b60c-d99b3e12b690"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/musica_pt-br.docx'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","# Aplica√ß√£o na pr√°tica com o Azure OpenAi\n","\n","---\n"],"metadata":{"id":"zIFwS-c8WyYL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z5I06gISFTd2","executionInfo":{"status":"ok","timestamp":1730909898493,"user_tz":180,"elapsed":11708,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"outputId":"8b2c4200-cead-4e5b-afc3-85d0fd056bd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n","Collecting langchain-openai\n","  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n","Collecting langchain-core<0.4.0,>=0.3.15 (from langchain-openai)\n","  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n","Collecting openai\n","  Downloading openai-1.54.2-py3-none-any.whl.metadata (24 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain-openai)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.33)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (0.1.137)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (24.1)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain-openai) (9.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (3.10.10)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-openai) (1.0.0)\n","Downloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.54.2-py3-none-any.whl (389 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m389.3/389.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-core, langchain-openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.52.2\n","    Uninstalling openai-1.52.2:\n","      Successfully uninstalled openai-1.52.2\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.13\n","    Uninstalling langchain-core-0.3.13:\n","      Successfully uninstalled langchain-core-0.3.13\n","Successfully installed langchain-core-0.3.15 langchain-openai-0.2.6 openai-1.54.2 tiktoken-0.8.0\n"]}],"source":["!pip install requests beautifulsoup4 openai langchain-openai"]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","def extract_from_url(url):\n","    response = requests.get(url)\n","\n","    if response.status_code == 200:\n","      soup = BeautifulSoup(response.text, 'html.parser')\n","      for script_or_style in soup([\"script\", \"style\"]):\n","        script_or_style.decompose\n","      texto = soup.get_text(separator= ' ')\n","      #limpar texto\n","      linhas = (line.strip() for line in texto.splitlines())\n","      parts = (phrase.strip() for line in linhas for phrase in line.split(\"  \"))\n","      texto_limpo = '\\n'.join(part for part in parts if part)\n","      return texto_limpo\n","    else:\n","      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n","      return None\n","\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","    text = soup.get_text()\n","    return text\n","\n","extract_from_url('https://dev.to/kenakamu/azure-open-ai-in-vnet-3alo')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184},"id":"XS9eKkOLHNxE","executionInfo":{"status":"ok","timestamp":1730909902926,"user_tz":180,"elapsed":935,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"outputId":"a635ff62-cef1-420a-fc03-7a9e18746148"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Azure Open AI in VNet - DEV Community\\nSkip to content\\nNavigation menu\\nSearch\\nPowered by\\nSearch\\nAlgolia\\nSearch\\nLog in\\nCreate account\\nDEV Community\\nClose\\nAdd reaction\\nLike\\nUnicorn\\nExploding Head\\nRaised Hands\\nFire\\nJump to Comments\\nSave\\nMore...\\nCopy link\\nCopy link\\nCopied to Clipboard\\nShare to X\\nShare to LinkedIn\\nShare to Facebook\\nShare to Mastodon\\nReport Abuse\\nKenichiro Nakamura\\nPosted on\\nOct 12, 2023\\nAzure Open AI in VNet\\n# azure\\n# openai\\n# security\\nGPT models are hosted in multiple service vendor at the moment, and Microsoft Azure is one of them.\\nEven though the models themselves are the same, there are many differences including:\\ncost\\nfunctionalities\\ntype of models and versions\\ngeo location\\nsecurity\\nsupport\\netc.\\nOne of the most important aspects when we use it in an Enterprise Environment is, of course, security.\\nBy using Azure network security features with Azure Open AI, customers can consume the Open AI service from and within the VNet, therefore no information is flowing in public.\\nSample Deployment\\nAzure Sample repo provides a sample bicep files to deploy Azure Open AI into VNet environment.\\nGitHub: openai-enterprise-iac\\nThe key features the bicep uses are:\\nVNet\\nVNet integration for Web App\\nPrivate Endpoint for Azure Open AI\\nPrivate Endpoint for Cognitive Search\\nPrivate DNS Zone\\nBy using these features, all the outbound traffic from the Web App only routed inside the VNet and all the names are resolved into private IP addresses. Open AI and Cognitive Search shut down the public IP address, thus there is not public interface endpoint available anymore.\\nDeploy\\nThe bicep file will deploy following Azure Resources.\\nLet's deploy and confirm how it works. I create a resource group in East US region for my own test.\\ngit clone https://github.com/Azure-Samples/openai-enterprise-iac\\ncd\\nopenai-enterprise-iac\\naz group create\\n-n\\nopenaitest\\n-l\\neastus\\naz deployment group create\\n-g\\nopenaitest\\n-f\\n. \\\\i nfra \\\\m ain.bicep\\nEnter fullscreen mode\\nExit fullscreen mode\\nOnce I run the commend above, I see the deployment started.\\nWait until the deployment completes.\\nTest\\nLet's see if the deployment was succeeded.\\nAzure Open AI\\nLet's try public access first.\\nI could create a deployment without any issue. But when I try from the Chat playground in my Azure Portal, I see the following error.\\nHow about access via the Web API?\\nFrom an advanced tool of the App Service, I login to Bash session, and first I ping the service URL.\\nI see the private IP address assigned to the Private Endpoint is returend.\\nThen I use curl command to send request to the endpoint.\\nTop comments\\n(0)\\nSubscribe\\nPersonal\\nTrusted User\\nCreate template\\nTemplates let you quickly answer FAQs or store snippets for re-use.\\nSubmit\\nPreview\\nDismiss\\nCode of Conduct\\n‚Ä¢\\nReport abuse\\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's\\npermalink .\\nHide child comments as well\\nConfirm\\nFor further actions, you may consider blocking this person and/or\\nreporting abuse\\nRead next\\nHow to create an image authentication system with python streamlit and canva!\\nYassine Sallami -\\nOct 23\\nIntroducing the DNA-KEY System: Taking the Password Generator to the Next Level! üîê\\nYassine Sallami -\\nOct 23\\nIntroducing Stable Diffusion 3.5\\nChristianHappyGo -\\nOct 23\\nUnderstanding Git Tags and Checkout\\nCiCube -\\nNov 4\\nKenichiro Nakamura\\nFollow\\nJoined\\nFeb 3, 2018\\nMore from\\nKenichiro Nakamura\\nAzure ML Prompt flow: Use content safety before sending a request to LLM\\n# azure\\n# promptflow\\n# contentsafety\\nDon't waste time to write README, use readme-ai instead\\n# ai\\n# readme\\n# openai\\nC#: Azure Open AI and Function Calling\\n# azure\\n# openai\\n# functioncalling\\nThank you to our Diamond Sponsor\\nNeon\\nfor supporting our community.\\nDEV Community\\n‚Äî A constructive and inclusive social network for software developers. With you every step of your journey.\\nHome\\nDEV++\\nPodcasts\\nVideos\\nTags\\nDEV Help\\nForem Shop\\nAdvertise on DEV\\nDEV Challenges\\nDEV Showcase\\nAbout\\nContact\\nFree Postgres Database\\nGuides\\nSoftware comparisons\\nCode of Conduct\\nPrivacy Policy\\nTerms of use\\nBuilt on\\nForem\\n‚Äî the\\nopen source\\nsoftware that powers\\nDEV\\nand other inclusive communities.\\nMade with love and\\nRuby on Rails . DEV Community\\n¬©\\n2016 - 2024.\\nWe're a place where coders share, stay up-to-date and grow their careers.\\nLog in\\nCreate account\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["from langchain_openai.chat_models import AzureChatOpenAI\n","\n","client = AzureChatOpenAI(\n","    azure_endpoint= \"https://openia-teste.openai.azure.com/\",\n","    api_key= \"9d3zIPcWN9yYlskzhZ1tj9KYJcC36iiFyTOm4EFzsHB7IHQ90NafJQQJ99AKACYeBjFXJ3w3AAABACOG2gGF\",\n","    api_version= \"2023-03-15-preview\",\n","    deployment_name= \"gpt-4\",\n","    max_retries=0\n",")\n","\n","def translate_article(text, lang):\n","  messages= [\n","      (\"system\", \"Voc√™ atua como tradutor de textos\"),\n","      (\"user\", f\"traduza o {text} para o idioma {lang} e responda em markdown\"),\n","  ]\n","\n","  response = client.invoke(messages)\n","  print(response.content)\n","  return response.content\n","\n","translate_article(\"GPT models are hosted in multiple service vendor at the moment, and Microsoft Azure is one of them.\", \"portugues\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"ei8nuNcHKeE6","executionInfo":{"status":"ok","timestamp":1730910189523,"user_tz":180,"elapsed":2368,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"outputId":"561e228b-d6d8-4bd5-d237-570a7a4559c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["```markdown\n","Os modelos GPT est√£o hospedados em m√∫ltiplos fornecedores de servi√ßo atualmente, e a Microsoft Azure √© um deles.\n","```\n"]},{"output_type":"execute_result","data":{"text/plain":["'```markdown\\nOs modelos GPT est√£o hospedados em m√∫ltiplos fornecedores de servi√ßo atualmente, e a Microsoft Azure √© um deles.\\n```'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["url = 'https://dev.to/kenakamu/azure-open-ai-in-vnet-3alo'\n","text = extract_from_url(url)\n","article = translate_article(text, \"pt-br\")\n","\n","print(article)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"kgqztCWk7Wo8","executionInfo":{"status":"error","timestamp":1730910246888,"user_tz":180,"elapsed":972,"user":{"displayName":"Agatha Ruiz","userId":"15415536774518208367"}},"outputId":"f5ea6438-549c-4dec-a97d-4ec2b9cb70e9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RateLimitError","evalue":"Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the Chatcompletions_Create Operation under Azure OpenAI API version 2023-03-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-40fca4e7de6a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://dev.to/kenakamu/azure-open-ai-in-vnet-3alo'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_article\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt-br\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-8e43f4da1e54>\u001b[0m in \u001b[0;36mtranslate_article\u001b[0;34m(text, lang)\u001b[0m\n\u001b[1;32m     15\u001b[0m   ]\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m         return cast(\n\u001b[1;32m    285\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    287\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    785\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     async def agenerate_prompt(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         flattened_outputs = [\n\u001b[1;32m    645\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 results.append(\n\u001b[0;32m--> 633\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    634\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    852\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    827\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         )\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    955\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         return self._process_response(\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the Chatcompletions_Create Operation under Azure OpenAI API version 2023-03-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 86400 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}"]}]}]}